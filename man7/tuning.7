.\" Copyright (C) 2001 Matthew Dillon. All rights reserved.
.\" Copyright (C) 2012 Eitan Adler.
.\"
.\" Redistribution and use in source and binary forms, with or without
.\" modification, are permitted provided that the following conditions
.\" are met:
.\" 1. Redistributions of source code must retain the above copyright
.\"    notice, this list of conditions and the following disclaimer.
.\" 2. Redistributions in binary form must reproduce the above copyright
.\"    notice, this list of conditions and the following disclaimer in the
.\"    documentation and/or other materials provided with the distribution.
.\"
.\" THIS SOFTWARE IS PROVIDED BY AUTHOR AND CONTRIBUTORS ``AS IS'' AND
.\" ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
.\" IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
.\" ARE DISCLAIMED.  IN NO EVENT SHALL AUTHOR OR CONTRIBUTORS BE LIABLE
.\" FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
.\" DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
.\" OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
.\" HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
.\" LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
.\" OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
.\" SUCH DAMAGE.
.\"
.Dd November 17, 2023
.Dt TUNING 7
.Os
.Sh ИМЯ
.Nm tuning
.Nd настройка производительности под FreeBSD
.Sh НАСТРОЙКА СИСТЕМЫ - МЕТКА ДИСКА, НОВЫЕ ФАЙЛЫ, НАСТРОЙКИ, ПОДКАЧКА
Размер раздела подкачки, как правило, должен быть примерно в 2 раза больше размера основной памяти для систем с объемом оперативной памяти менее 4 ГБ или примерно равен размеру основной памяти, если у вас больше.
При выборе размера раздела подкачки учитывайте возможность расширения памяти в будущем.
Настройка слишком малого объема подкачки может привести к неэффективности кода сканирования страниц виртуальной машины, а также к возникновению проблем в дальнейшем, если вы увеличите объем памяти на своем компьютере.
В больших системах с несколькими дисками настройте подкачку на каждом диске.
Разделы подкачки на дисках должны быть примерно одинакового размера.
Ядро может обрабатывать данные произвольного размера, но внутренние структуры данных масштабируются в 4 раза больше размера раздела подкачки.
Сохранение разделов подкачки примерно одинакового размера позволит ядру оптимально распределить пространство подкачки между N дисками.
Не беспокойтесь о
том, чтобы немного не перестараться, пространство для подкачки - это спасительная возможность
.Ux
и даже если вы обычно не используете много подкачки, это может дать вам больше времени для
восстановления после сбоя программы, прежде чем вы будете вынуждены перезагрузиться.
.Pp
Создавать один большой раздел - не очень хорошая идея.
Во-первых, каждый раздел имеет разные рабочие характеристики, и их разделение
позволяет файловой системе адаптироваться к этим характеристикам.
Например,
корневой каталог и разделы
.Pa /usr
в основном читаются, с очень небольшим количеством записей, в то время
как в
.Pa /var/tmp 
может происходить много чтения и записи.
При правильном
разбиении системы на разделы фрагментация, возникающая в небольших разделах с большей
нагрузкой на запись, не будет распространяться на разделы, которые в основном используются для чтения.
.Pp
Правильное разделение вашей системы на разделы также позволяет вам настроить
.Xr newfs 8 ,
и параметры
.Xr tunefs 8 .
Единственный вариант
.В Xr tunefs 8
стоит включить
.Em softupdates
с помощью
.Dq Li "tunefs -n enable /filesystem" .
Softupdates значительно повышает производительность работы с метаданными, в основном при создании и удалении файлов.
Мы рекомендуем включить softupdates в большинстве файловых систем; однако у программных обновлений есть два ограничения, о которых вам следует знать, когда вы решаете, использовать ли их в файловой системе.
Во-первых, softupdates гарантирует стабильность файловой системы в
случае сбоя, но очень легко может произойти задержка на несколько секунд (даже на минуту!\&) в ожидании записи на физический диск.
В случае сбоя вы можете потерять больше работы, чем в противном случае.
Во-вторых, программные обновления задерживают освобождение блоков файловой системы.
Если у вас есть файловая система (например, корневая файловая система), которая
близка к заполнению, выполните ее серьезное обновление, например,\&
.Dq Li "make installworld" ,
может не хватить места на диске, что приведет к сбою обновления.
По этой причине при обычной установке в корневой файловой системе не будут включены программные обновления.
Производительность не снижается, поскольку запись в корневую
файловую систему выполняется редко.
.Pp
Во время выполнения
.Xr mount 8
существует ряд опций, которые могут помочь вам настроить систему.
Самый очевидный и самый опасный из них - это
.Cm async .
Используйте эту опцию только в сочетании с
.Xr gjournal 8 ,
поскольку в обычной файловой системе это слишком опасно.
Менее опасный и более
полезный вариант
.Xr mount 8
называется
.Cm noatime .
.Ux
файловые системы обычно обновляют время последнего обращения к файлу или
каталогу при каждом обращении к ним.
Эта операция выполняется в
.Fx
с задержкой записи и, как правило, не создает нагрузки на систему.
Однако, если ваша система постоянно обращается к огромному количеству файлов,
буферный кэш может быть загрязнен обновлениями во времени,
что создает нагрузку на систему.
Например, если у вас сильно
загруженный веб-сайт или новостной сервер с большим количеством читателей, вы можете
рассмотреть возможность отключения обновлений atime на больших разделах с помощью опции
.Xr mount 8 .
Однако вам не следует беспричинно отключать временные
обновления повсеместно.
Например, файловая система
.Pa /var
обычно
содержит почтовые ящики, а время (в сочетании со значением time) используется для
определения того, есть ли в почтовом ящике новая почта.
Вы также можете оставить
время включенным для разделов, предназначенных в основном только для чтения, таких как
.Pa /
и также
.Pa /usr .
Это особенно полезно для
.Pa /
поскольку некоторые системные утилиты
используют поле atime для создания отчетов.
.Sh ЧЕРЕДОВАНИЕ ДИСКОВ
В больших системах вы можете разделить разделы с нескольких дисков,
чтобы создать общий раздел гораздо большего размера.
Разделение также может повысить
производительность файловой системы за счет разделения операций ввода-вывода на два
или более дисков.
Утилиты
.Xr gstripe 8 ,
.Xr gvinum 8 ,
и
.Xr ccdconfig 8
могут использоваться для создания простых чередующихся файловых систем.
Вообще говоря, разделение небольших разделов, таких как корневой и
.Pa /var/tmp ,
или, по сути, разделы, доступные только для чтения, такие как
.Pa /usr
это пустая трата времени.
Как правило, вы должны разделять разделы только на те, которые требуют высокой производительности ввода-вывода
.Pa /var , /home ,
или пользовательские разделы, используемые для хранения баз данных и веб-страниц.
Также важен выбор правильного размера разделов.

Файловые системы, как правило, хранят метаданные в ограниченных пределах,
и обычно требуется уменьшить количество запросов, а не увеличивать количество запросов.
Это
означает, что вы хотите использовать полосу большого размера, смещенную от центра, например, 1152 сектора,
чтобы последовательный ввод-вывод не обращался к обоим дискам и чтобы метаданные распределялись
по обоим дискам, а не концентрировались на одном диске.
.Sh НАСТРОЙКА СИСТЕМНОГО КОДА
.Xr sysctl 8
переменные позволяют отслеживать и контролировать поведение системы во
время выполнения.
Некоторые системные коды просто сообщают о поведении системы; другие позволяют
изменять поведение системы.;
некоторые из них могут быть установлены во время загрузки с помощью
.Xr rc.conf 5 ,
но большинство из них будет установлено с помощью
.Xr sysctl.conf 5 .
В системе имеется несколько сотен системных каталогов, многие из которых кажутся
кандидатами на настройку, но на самом деле таковыми не являются.
В этом документе мы рассмотрим только те, которые оказывают наибольшее влияние
на систему.
.Pp
Системный код
.Va vm.overcommit
определяет режим избыточной загрузки подсистемы виртуальной машины.
Система виртуальной памяти всегда выполняет учет
резервирования пространства подкачки, как общего для системы, так и для каждого пользователя.
Соответствующие значения
доступны через системный код
.Va vm.swap_total ,
который дает общее количество байт, доступных для обмена, и
.Va vm.swap_reserved ,
который дает количество байт, которое может потребоваться для резервного
копирования всей выделенной в данный момент анонимной памяти.
.Pp
Установка бита 0 в системном коде
.Va vm.overcommit
приводит к тому, что система виртуальной памяти возвращает сбой
процессу, когда выделение памяти приводит к превышению
.Va vm.swap_reserved
.Va vm.swap_total .
Бит 1 системного кода устанавливает ограничение
.Dv RLIMIT_SWAP
(смотрите
.Xr getrlimit 2 ) .
Root не попадает под это ограничение.
Бит 2 позволяет считать большую часть физической
памяти доступной для выделения, за исключением подключенных и свободных зарезервированных страниц
(учитываемые системными кодами
.Va vm.stats.vm.v_free_target
и
.Va vm.stats.vm.v_wire_count ,
соответственно).
.Pp
Настраиваемый загрузчик
.Va kern.ipc.maxpipekva
используется для установки жесткого ограничения на
объем адресного пространства ядра, выделяемого для отображения буферов канала.
Использование отображения позволяет ядру исключить копирование
данных из адресного пространства записи в ядро, напрямую копируя
содержимое отображенного буфера в считывающее устройство.
Увеличение этого значения до более высокого, например до "25165824", может
повысить производительность в системах, где пространство для отображения буферов каналов
быстро исчерпывается.
Однако это истощение не является фатальным и приведет лишь к тому, что каналы
вернутся к использованию двойного копирования.
.Pp
Системный код
.Va kern.ipc.shm_use_phys
имеет значение по умолчанию равное 0 (выкл.), но может быть изменено на 0 (выкл.) или 1 (вкл.).
Установка
этого параметра равным 1 приведет к тому, что все сегменты совместно используемой памяти System V будут
преобразованы в физическую оперативную память, доступ к которой невозможен.
Эта функция работает только в том случае, если вы
либо (А) распределяете небольшие объемы общей памяти между многими (сотнями)
процессами, либо (Б) распределяете большие объемы общей памяти между любым
количеством процессов.
Эта функция позволяет ядру избавиться от значительных
затрат на управление внутренней памятью, связанных с отслеживанием страниц, за счет подключения
общей памяти к ядру, что делает ее недоступной для замены.
.Pp
Системный код
.Va vfs.vmiodirenable
имеет значение по умолчанию равное 1 (включено).
Этот параметр определяет, как система кэширует каталоги.
Большинство каталогов небольшого размера и используют всего один фрагмент
(обычно 2 КБ) в файловой системе и еще меньше (обычно 512 байт) в
буферном кэше.
Однако при работе в режиме по умолчанию буферный
кэш будет кэшировать только фиксированное количество каталогов, даже если у вас большой
объем памяти.
Включение этого sysctl позволяет буферному кэшу использовать
кэш страниц виртуальной машины для кэширования каталогов.
Преимущество заключается в том, что теперь вся
память доступна для кэширования каталогов.
Недостатком является то, что
минимальный объем встроенной памяти, используемый для кэширования каталога, равен физическому
размеру страницы (обычно 4 КБ), а не 512 байтам.
Мы рекомендуем отключить эту опцию в средах с ограниченным объемом памяти;
однако, если он включен, это существенно повысит производительность служб,
которые обрабатывают большое количество файлов.
Такие службы могут включать веб-кэши, крупные почтовые системы и новостные системы.
Включение этой опции, как правило, не приводит к снижению производительности даже при
потере памяти, но вам следует поэкспериментировать, чтобы выяснить это.
.Pp
Системный код
.Va vfs.write_behind
имеет значение по умолчанию равное 1 (включено).
Это указывает файловой системе на необходимость
записи на носитель по мере сбора полных кластеров, что обычно происходит при записи
больших последовательных файлов.
Идея заключается в том, чтобы избежать переполнения буферного
кэша грязными буферами, когда это не улучшит производительность ввода-вывода.
Однако
это может привести к остановке процессов, и при определенных обстоятельствах вы можете захотеть отключить
его.
.Pp
Системный код
.Va vfs.hirunningspace
определяет, сколько незавершенных операций ввода-вывода для записи может быть поставлено в очередь на
контроллеры дисков в масштабе всей системы в любой момент времени.
Используется файловой системой UFS.
Значение по умолчанию настраивается автоматически и
обычно этого достаточно, но на компьютерах с продвинутыми контроллерами и большим
количеством дисков это значение может быть изменено в соответствии с тем, что буферизуют контроллеры.
Настройка этого параметра в соответствии с возможностями организации очереди с тегами
для контроллеров или накопителей со средним размером ввода-вывода, используемым в рабочей среде, работает
лучше всего (например, 16 Мб будет использовать 128 тегов при запросах ввода-вывода размером 128 КБ).
Обратите внимание, что слишком большое значение
(превышение порога записи в буферный кэш) может привести к крайне
низкой производительности кластеризации.
Не устанавливайте это значение произвольно высоким!
Более высокие значения очереди записи также могут увеличить задержку одновременного чтения.
.Pp
Системный код
.Va vfs.read_max
управляет процессом опережающего чтения VFS и выражается как количество блоков
для предварительного чтения, если эвристический алгоритм решает, что чтение
выполняется последовательно.
Используется файловыми системами UFS, ext2fs и msdosfs.
При размере блока UFS по умолчанию 32 Кбайт, значение 64 позволит
считывать данные размером до 2 Кбайт.
Этот параметр может быть увеличен для уменьшения задержек ввода-вывода с диска, особенно
в тех случаях, когда эти задержки велики, например, в средах, эмулируемых виртуальными машинами.
В особых случаях, когда нагрузка на ввод-вывод такова, что
опережающее чтение отрицательно сказывается на производительности или в тех случаях, когда системной памяти действительно
мало.
.Pp
Системный код
.Va vfs.ncsizefactor
определяет, насколько может увеличиться объем кэша имен VFS.
Количество выделенных в данный момент записей в кэше имен определяется системным кодом
.Va debug.numcache
и состояние
debug.numcache < kern.maxvnodes * vfs.ncsizefactor
соблюдается.
.Pp
Системный код
.Va vfs.ncnegfactor
определяет, сколько отрицательных записей разрешено создавать VFS namecache.
Количество выделенных в данный момент отрицательных записей определяется системным кодом
.Va debug.numneg
и состояние
vfs.ncnegfactor * debug.numneg < debug.numcache
соблюдается.
.Pp
Существуют различные другие системные требования, связанные с буферным кэшем и кэшированием страниц виртуальной машины.
Мы не рекомендуем изменять эти значения.
.Pp
Системные коды
.Va net.inet.tcp.sendspace
и
.Va net.inet.tcp.recvspace
они представляют особый интерес, если вы используете
приложения с интенсивной работой в сети.
Они регулируют объем буферного пространства для отправки и приема,
разрешенный для любого данного TCP-соединения.
Буфер отправки по умолчанию равен 32 КБАЙТ, буфер приема по умолчанию
равен 64 Кбайт.
Часто можно
улучшить использование полосы пропускания, увеличив значение по умолчанию за счет
увеличения объема памяти ядра для каждого подключения.
Мы не рекомендуем
увеличивать значения по умолчанию, если вы обслуживаете сотни или тысячи
одновременных подключений, поскольку это может привести к быстрому запуску системы
из-за нехватки памяти из-за сбоев в подключениях.
Но если вам нужна
высокая пропускная способность при меньшем количестве подключений, особенно если у вас
гигабитный Ethernet, увеличение значений по умолчанию может иметь огромное значение.
Вы можете настроить размер буфера для входящих и исходящих данных отдельно.
Например, если ваша машина в основном занимается веб-обслуживанием, вы можете захотеть
уменьшить пространство записи, чтобы иметь возможность увеличить
пространство отправки, не занимая слишком много памяти ядра.
Обратите внимание, что таблицу маршрутизации (смотрите
.Xr route 8 )
можно использовать для введения значений размера буфера отправки и приема
по умолчанию для конкретного маршрута.
.Pp
В качестве дополнительного инструмента управления вы можете использовать каналы в своих
правилах брандмауэра (смотрите
.Xr ipfw 8 )
чтобы ограничить пропускную способность, идущую к определенным IP-блокам или портам.
Например, если у вас есть T1, вы можете ограничить свой веб-трафик
до 70% от пропускной способности T1, чтобы оставшаяся часть была доступна
для почты и интерактивного использования.
Обычно сильно загруженный веб-сервер
не приводит к значительным задержкам в работе других служб, даже если
сетевое соединение максимально загружено, но введение ограничения может сгладить ситуацию
и обеспечить долгосрочную стабильность.
Многие люди также вводят искусственные
ограничения полосы пропускания, чтобы с них не взимали плату за
использование слишком большой полосы пропускания.
.Pp
Установка для буфера отправки или получения TCP значений, превышающих 65535, приведет
к незначительному повышению производительности, если только оба хоста не поддерживают
расширение масштабирования окна протокола TCP, которое управляется системным кодом
.Va net.inet.tcp.rfc1323 .
Эти расширения должны быть включены, а размер буфера TCP должен быть установлен
на значение, превышающее 65536, чтобы обеспечить хорошую производительность
определенных типов сетевых соединений, в частности, гигабитных WAN-соединений и
спутниковых соединений с высокой задержкой.
Поддержка RFC1323 включена по умолчанию.
.Pp
Системный код
.Va net.inet.tcp.always_keepalive
определяет, должна ли реализация TCP пытаться
обнаружить неработающие TCP-соединения путем периодической доставки
.Dq keepalives
на соединения.
По умолчанию это включено для всех приложений; установив для этого
системного кода значение 0, их будут использовать только те приложения, которые специально запрашивают keepalive.
В большинстве сред поддержка TCP keepalives улучшит управление состоянием
системы за счет прекращения действия недействительных TCP-соединений, особенно для
систем, обслуживающих удаленных пользователей, которые не всегда могут завершить отдельные соединения.
TCP-соединения перед отключением от сети.
Однако в некоторых средах временные сбои в работе сети могут быть
неправильно идентифицированы как неработающие сеансы, что приводит к неожиданному
завершению TCP-соединений.
В таких средах установка значения системного кода равным 0 может снизить вероятность
прерывания сеанса TCP.
.Pp
Функция TCP
.Va net.inet.tcp.delayed_ack
во многом это неправильно понимается.
Исторически сложилось так, что эта функция
была разработана для того, чтобы позволить возвращать подтверждение передаваемых данных
вместе с ответом.
Например, когда вы вводите текст через удаленную командную строку,
подтверждение отправляемого вами символа может быть возвращено вместе с
данными, представляющими эхо-сигнал этого символа.
При отключенной функции отложенного
подтверждения подтверждение может быть отправлено отдельным пакетом, прежде чем удаленная служба
получит возможность повторить только что полученные данные.
Эта же концепция
применима к любому интерактивному протоколу (например, \& SMTP, WWW, POP3) и может сократить
количество крошечных пакетов, проходящих по сети, сокращается вдвое.
Отложенная реализация подтверждения
.Fx
также соблюдает правило протокола TCP, согласно которому
по крайней мере каждый второй пакет должен быть подтвержден, даже если стандартный
тайм-аут в 40 мс еще не истек.
Обычно самое худшее, к чему может привести задержка подтверждения, - это
небольшая задержка разрыва соединения или небольшая задержка запуска
TCP-соединения с медленным запуском.
Хотя мы не уверены, мы полагаем, что
несколько часто задаваемых вопросов, связанных с такими пакетами, как SAMBA и SQUID, в которых рекомендуется
отключить отложенные подтверждения, могут относиться к проблеме медленного запуска.
.Pp
Системные коды
.Va net.inet.ip.portrange.*
контролируйте диапазоны номеров портов, автоматически привязываемые к сокетам TCP и UDP.
Существует три диапазона: низкий диапазон, диапазон по умолчанию и
высокий диапазон, которые можно выбрать с помощью вызова
.Dv IP_PORTRANGE
.Xr setsockopt 2 .
Большинство
сетевых программ используют диапазон по умолчанию, который контролируется
.Va net.inet.ip.portrange.first
и
.Va net.inet.ip.portrange.last ,
которые по умолчанию равны 49152 и 65535 соответственно.
Для исходящих подключений используются ограниченные диапазоны портов, и при определенных обстоятельствах можно запускать систему без
использования портов.
Чаще всего это происходит, когда вы
используете сильно загруженный веб-прокси.
Диапазон портов не является проблемой
при работе сервера, который обрабатывает в основном входящие соединения, такие как
обычный веб-сервер, или имеет ограниченное количество исходящих соединений, таких
как почтовый ретранслятор.
В ситуациях, когда у вас могут закончиться порты,
мы рекомендуем немного уменьшить
.Va net.inet.ip.portrange.first .
Допустимым может быть диапазон от 10000 до 30000 портов.
При изменении диапазона портов следует также учитывать влияние брандмауэра.
Некоторые брандмауэры
могут блокировать большие диапазоны портов (обычно с низким номером) и ожидать, что системы
будут использовать более высокие диапазоны портов для исходящих подключений.
По умолчанию
.Va net.inet.ip.portrange.last
устанавливается максимально допустимый номер порта.
.Pp
Системный код
.Va kern.ipc.soacceptqueue
ограничивает размер очереди прослушивания для приема новых TCP-подключений.
Значение по умолчанию 128 обычно слишком мало для надежной обработки новых
подключений в среде веб-сервера с высокой нагрузкой.
Для таких сред
рекомендуется увеличить это значение до 1024 или выше.
Сервисный демон
может сам ограничить размер очереди прослушивания (например,\&
.Xr sendmail 8 ,
apache) но
часто в файле конфигурации есть директива, позволяющая увеличить размер очереди.
Большие очереди прослушивания также лучше справляются с отражением атак типа отказа в обслуживании.
.Pp
Системный код
.Va kern.maxfiles
определяет, сколько открытых файлов поддерживает система.
Обычно значение по умолчанию равно нескольким тысячам, но вам может потребоваться увеличить это значение до десяти или двадцати
тысяч, если вы используете базы данных или демонов с большим количеством дескрипторов.
Системный код
.Va kern.openfiles ,
доступный только для чтения
может быть запрошен для определения текущего количества открытых файлов
в системе.
.Pp
Системный код
.Va vm.swap_idle_enabled
это полезно в больших многопользовательских системах, где множество пользователей
входят в систему и выходят из нее, а также множество незанятых процессов.
Такие системы
, как правило, создают значительную постоянную нагрузку на свободные ресурсы памяти.
Включение этой функции и регулировка гистерезиса замены (в
секундах простоя) с помощью
.Va vm.swap_idle_threshold1
и
.Va vm.swap_idle_threshold2
позволяет снизить приоритет страниц, связанных с незанятыми процессами,
быстрее, чем при использовании обычного алгоритма вывода страниц.
Это помогает
демону вывода страниц.
Не включайте эту опцию, если она вам не нужна,
потому что компромисс, на который вы идете, заключается в том, что вы, по сути, раньше загружаете память до загрузки страниц,
а не позже, что увеличивает объем подкачки и пропускную способность диска.
В небольшой системе
этот параметр будет иметь отрицательный эффект, но в большой системе, которая
уже выполняет умеренную подкачку, этот параметр позволяет системе виртуальной машины легче загружать
целые процессы в память и извлекать их из нее.
.Sh НАСТРОЙКА ЗАГРУЗЧИКА
Некоторые аспекты поведения системы могут быть недоступны для настройки во время выполнения, поскольку
выделение памяти, которое они выполняют, должно происходить на ранней стадии процесса загрузки.
Чтобы изменить настройки загрузчика, необходимо задать их значения в
.Xr loader.conf 5
и перезагрузить систему.
.Pp
.Va kern.maxusers
управляет масштабированием ряда статических системных таблиц, включая значения по умолчанию
для максимального количества открытых файлов, размер ресурсов сетевой памяти, и т.д.
Размер
.Va kern.maxusers
автоматически определяется при загрузке в зависимости от объема доступной в
системе памяти и может быть определен во время выполнения путем проверки значения
параметра системного кода
.Va kern.maxusers ,
предназначенного только для чтения.
.Pp
Настраиваемые
.Va kern.dfldsiz
и
.Va kern.dflssiz
устанавливают мягкие ограничения по умолчанию для данных процесса и размера стека
соответственно.
Процессы могут увеличить их до жестких пределов, вызвав
.Xr setrlimit 2 .
Настраиваемые
.Va kern.maxdsiz ,
.Va kern.maxssiz ,
и
.Va kern.maxtsiz
устанавливают жесткие ограничения для данных процесса, размера стека и текста
соответственно; процессы не должны превышать эти ограничения.
Настраиваемый
.Va kern.sgrowsiz
определяет, насколько увеличится сегмент стека, когда процессу
потребуется выделить больше стека.
.Pp
.Va kern.ipc.nmbclusters
может быть скорректировано для увеличения количества сетевых mbuf, которые система
готова выделить.
Каждый кластер занимает приблизительно 2 КБАЙТ памяти,
поэтому значение 1024 соответствует 2 мбайт памяти ядра, зарезервированной для сетевых
буферов.
Вы можете произвести простой расчет, чтобы определить, сколько их вам нужно.
Если у вас есть веб-сервер, который обеспечивает максимум 1000 одновременных подключений,
и каждое подключение потребляет 16 КБ буфера приема и 16 КБ буфера отправки, вам потребуется
около 32 МБ сетевых буферов, чтобы справиться с этим.
Хорошее
эмпирическое правило - умножить на 2, таким образом, 32 Мбит/с2 = 64 МБ/2 КБ = 32768.
Таким образом, в этом случае
вы хотели бы установить
.Va kern.ipc.nmbclusters
значение 32768.
Мы рекомендуем значений между
1024 и 4096 для машин с умеренными объем памяти, а между 4096
и 32768 для машин с большим объемом памяти.
Ни при каких обстоятельствах
не следует указывать сколь угодно высокое значение для этого параметра, это может
привести к сбою во время загрузки.
Параметр
.Fl m
к
.Xr netstat 1
может использоваться для наблюдения за использованием сетевого кластера.
.Pp
Все больше и больше программ используют систему
.Xr sendfile 2
для вызов передачи файлов по сети.
Системный код
.Va kern.ipc.nsfbufs
управляет количеством буферов файловой системы
.Xr sendfile 2
разрешается использовать для выполнения своей работы.
Номинал этого параметра измеряется
с помощью
.Va kern.maxusers ,
таким образом, вам не нужно будет изменять этот параметр, за исключением чрезвычайных
обстоятельств.
Смотрите раздел
.Sx НАСТРОЙКА
на странице руководства
.Xr sendfile 2
для получения подробной информации.
.Sh НАСТРОЙКА КОНФИГУРАЦИИ ЯДРА
Существует ряд параметров ядра, с которыми вам, возможно, придется повозиться в
крупномасштабной системе.
Чтобы изменить эти параметры, вам необходимо иметь
возможность скомпилировать новое ядро из исходного кода.
Страница руководства
.Xr config 8
и руководство по эксплуатации являются хорошей отправной точкой для изучения того, как
это сделать.
Как правило, первое, что вы делаете при создании собственного пользовательского
ядра, - это удаляете все драйверы и службы, которые вы не используете.
Удаляя такие вещи, как
.Dv INET6 ,
а драйверы, которых у вас нет, уменьшат размер вашего ядра, иногда
на мегабайт или больше, оставляя больше памяти доступной для приложений.
.Pp
.Dv SCSI_DELAY
может использоваться для сокращения времени загрузки системы.
Значения по умолчанию довольно высокие и
могут приводить к задержке процесса загрузки более чем на 5 секунд.
Сокращение
.Dv SCSI_DELAY ,
чтобы что-то могло сработать менее чем за 5 секунд (особенно с современными приводами).
.Pp
Есть несколько параметров
.Dv *_CPU ,
которые можно прокомментировать.
Если вы хотите, чтобы ядро работало только
на процессоре класса Pentium, вы можете легко удалить
.Dv I486_CPU ,
но только удалить
.Dv I586_CPU
если вы уверены, что ваш процессор распознается как Pentium II или выше.
Некоторые клоны могут быть распознаны как Pentium или даже 486 и не смогут
загрузиться без этих опций.
Если это работает, отлично!
Операционная система
сможет лучше использовать возможности процессора более высокого класса для MMU, переключения задач,
учета времени и даже работы с устройствами.
Кроме того, процессоры более высокого класса поддерживают
MMU-страницы объемом 4 МБ, которые ядро использует для отображения самого ядра в памяти,
повышая его эффективность при больших нагрузках системного вызова.
.Sh ПРОЦЕССОР, ПАМЯТЬ, ДИСК, СЕТЬ
Тип настройки, которую вы выполняете, в значительной степени зависит от того, где ваша система начинает
испытывать проблемы с увеличением нагрузки.
Если в вашей системе не хватает процессора (время простоя постоянно
равно 0%), то вам нужно подумать об обновлении процессора
или, возможно, вам нужно пересмотреть
программы, которые вызывают нагрузку, и попытаться оптимизировать их.
Если ваша система
часто выполняет подкачку страниц, вам нужно рассмотреть возможность увеличения объема памяти.
Если ваша
система перегружает диск, вы, как правило, сталкиваетесь с большим временем простоя процессора и
общей загруженностью диска.
.Xr systat 1
можно использовать для отслеживания этого.
Существует множество решений проблемы перегруженных дисков:
увеличение объема памяти для кэширования, зеркальное отображение дисков, распределение операций между
несколькими компьютерами и так далее.
.Pp
Наконец, у вас может закончиться сетевая пена.
Максимально оптимизируйте сетевой путь.
Например, в
.Xr firewall 7
мы описываем брандмауэр, защищающий внутренние хосты, с топологией, в которой
внешние видимые хосты не маршрутизируются через него.
Большинство узких мест возникает в глобальной сети.
Если расширение сети невозможно, возможно, можно использовать эту функцию
.Xr dummynet 4
для снижения пиковой нагрузки или других форм формирования трафика,
чтобы перегруженный сервис (например, веб-службы) не влиял на другие
сервисы (например, электронную почту), или наоборот.
В домашних установках это может
использоваться для обеспечения интерактивного трафика (ваш браузер, логины
.Xr ssh 1 )
приоритет
над сервисами, которые вы экспортируете из своего почтового ящика (веб-сервисы, электронная почта).
.Sh СМОТРИТЕ ТАКЖЕ
.Xr netstat 1 ,
.Xr systat 1 ,
.Xr sendfile 2 ,
.Xr ata 4 ,
.Xr dummynet 4 ,
.Xr eventtimers 4 ,
.Xr login.conf 5 ,
.Xr rc.conf 5 ,
.Xr sysctl.conf 5 ,
.Xr ffs 7 ,
.Xr firewall 7 ,
.Xr hier 7 ,
.Xr ports 7 ,
.Xr boot 8 ,
.Xr bsdinstall 8 ,
.Xr ccdconfig 8 ,
.Xr config 8 ,
.Xr fsck 8 ,
.Xr gjournal 8 ,
.Xr gpart 8 ,
.Xr gstripe 8 ,
.Xr gvinum 8 ,
.Xr ifconfig 8 ,
.Xr ipfw 8 ,
.Xr loader 8 ,
.Xr mount 8 ,
.Xr newfs 8 ,
.Xr route 8 ,
.Xr sysctl 8 ,
.Xr tunefs 8
.Sh ИСТОРИЯ
Страница руководства
.Nm
первоначально была написана
.An Мэтью Диллоном
и впервые появился
в
.Fx 4.3 ,
в мае 2001 года.
Страница руководства была значительно изменена
.An Итаном Адлером Aq Mt eadler@FreeBSD.org .